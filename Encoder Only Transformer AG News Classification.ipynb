{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtwTIjo7oOuB"
   },
   "source": [
    "# Encoder-only transformer model for AG News classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "o5eW3azllzRd"
   },
   "source": [
    "In this notebook, I train a encoder-only transformer to do text classification on the AG_NEWS dataset.\n",
    "Text classification seems to be a pretty simple task, and using transformer is probably overkill. But this is my first time implementing the transformer structure from scratch (including the self-attention module), and it was fun :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Mtq3abS2lL_i"
   },
   "outputs": [],
   "source": [
    "# some commands in th is notebook require torchtext 0.12.0\n",
    "!pip install  torchtext --upgrade --quiet\n",
    "!pip install torchdata --quiet\n",
    "!pip install torchinfo --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1650896386699,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "bQEBuaIm5s9R",
    "outputId": "ccbc67d9-5437-43f6-e385-740925cbe04a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "dlopen(/opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: <7E3C8144-0701-3505-8587-6E953627B6AF> /opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <5445D2E4-6D7A-39F2-9003-F3A3F854555A> /opt/anaconda3/lib/python3.12/site-packages/torch/lib/libc10.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfunctional\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchdata\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m _load_lib(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibtorchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mload_library(path)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_ops.py:1357\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1352\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mCDLL(path)\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN3c105ErrorC1ENSt3__112basic_stringIcNS1_11char_traitsIcEENS1_9allocatorIcEEEES7_PKv\n  Referenced from: <7E3C8144-0701-3505-8587-6E953627B6AF> /opt/anaconda3/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     <5445D2E4-6D7A-39F2-9003-F3A3F854555A> /opt/anaconda3/lib/python3.12/site-packages/torch/lib/libc10.dylib"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import torchtext\n",
    "import torchdata\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmM0VcYnCC8M"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2641,
     "status": "ok",
     "timestamp": 1650896389339,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "7UdYsN6J5uke"
   },
   "outputs": [],
   "source": [
    "# One can easily modify the data processing part of this code to accommodate for   other datasets for text classification listed in https://pytorch.org/text/stable/datasets.html#text-classification\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "num_classes = len(set([label for (label, text) in train_iter]))\n",
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1650896389339,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "HchVgEXWlqLz",
    "outputId": "5ed8c7ef-783b-4ffd-8724-7216b44c8c31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see an example of the dateset\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DeTWUptkYnG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert the labels to be in range(0, num_classes)\n",
    "y_train = torch.tensor([label-1 for (label, text) in train_iter])\n",
    "y_test  = torch.tensor([label-1 for (label, text) in test_iter])\n",
    "\n",
    "# There are many \"\\\\\" in the texts in the AG_news dataset, we get rid of them.\n",
    "train_iter = ((label, text.replace(\"\\\\\", \" \")) for label, text in train_iter)\n",
    "test_iter  = ((label, text.replace(\"\\\\\", \" \")) for label, text in test_iter)\n",
    "\n",
    "# tokenize the texts, and truncate the number of words in each text to max_seq_len\n",
    "max_seq_len = 100\n",
    "x_train_texts = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in train_iter]\n",
    "x_test_texts  = [tokenizer(text.lower())[0:max_seq_len]\n",
    "                 for (label, text) in test_iter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1650896399478,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "MYVE8HSGkYnH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build the vocabulary and word-to-integer map\n",
    "counter = collections.Counter()\n",
    "for text in x_train_texts:\n",
    "    counter.update(text)\n",
    "\n",
    "vocab_size = 15000\n",
    "most_common_words = np.array(counter.most_common(vocab_size - 2))\n",
    "vocab = most_common_words[:,0]\n",
    "\n",
    "# indexes for the padding token, and unknown tokens\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "word_to_id = {vocab[i]: i + 2 for i in range(len(vocab))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2463,
     "status": "ok",
     "timestamp": 1650896401940,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "I7-4KQI8kYnH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# map the words in the training and test texts to integers\n",
    "x_train = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "           for text in x_train_texts]\n",
    "x_test  = [torch.tensor([word_to_id.get(word, UNK) for word in text])\n",
    "          for text in x_test_texts]\n",
    "x_test = torch.nn.utils.rnn.pad_sequence(x_test,\n",
    "                                batch_first=True, padding_value = PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650896401940,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "hJe8LAUNkYnI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# constructing the dataset in order to be compatible with torch.utils.data.Dataloader\n",
    "class AGNewsDataset:\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item], self.labels[item]\n",
    "\n",
    "\n",
    "train_dataset = AGNewsDataset(x_train, y_train)\n",
    "test_dataset  = AGNewsDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650896401941,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "ov3tX4sRkYnI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# collate_fn to be used in torch.utils.data.DataLoader().\n",
    "# It pads the texts in each batch such that they have the same sequence length.\n",
    "def pad_sequence(batch):\n",
    "    texts  = [text for text, label in batch]\n",
    "    labels = torch.tensor([label for text, label in batch])\n",
    "    texts_padded = torch.nn.utils.rnn.pad_sequence(texts,\n",
    "                                batch_first=True, padding_value = PAD)\n",
    "    return texts_padded, labels\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True,\n",
    "                        collate_fn = pad_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNVoCKz0CM3g"
   },
   "source": [
    "## Building the encoder-only transformer model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650896401941,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "gxqdJRLXyHin"
   },
   "outputs": [],
   "source": [
    "# from transformer_blocks import Encoder\n",
    "# One can also import Encoder from transformer_blocks.py in my Github repository.\n",
    "# I copied the code here so that this notebook is self-contained.  \n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_embed, dropout=0.0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_embed % h == 0 # check the h number\n",
    "        self.d_k = d_embed//h\n",
    "        self.d_embed = d_embed\n",
    "        self.h = h\n",
    "        self.WQ = nn.Linear(d_embed, d_embed)\n",
    "        self.WK = nn.Linear(d_embed, d_embed)\n",
    "        self.WV = nn.Linear(d_embed, d_embed)\n",
    "        self.linear = nn.Linear(d_embed, d_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x_query, x_key, x_value, mask=None):\n",
    "        nbatch = x_query.size(0) # get batch size\n",
    "        # 1) Linear projections to get the multi-head query, key and value tensors\n",
    "        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n",
    "        # LHS query, key, value dimensions: nbatch * h * seq_len * d_k\n",
    "        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
    "        # 2) Attention\n",
    "        # scores has dimensions: nbatch * h * seq_len * seq_len\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_k)\n",
    "        # 3) Mask out padding tokens and future tokens\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        # p_atten dimensions: nbatch * h * seq_len * seq_len\n",
    "        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        p_atten = self.dropout(p_atten)\n",
    "        # x dimensions: nbatch * h * seq_len * d_k\n",
    "        x = torch.matmul(p_atten, value)\n",
    "        # x now has dimensions:nbtach * seq_len * d_embed\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n",
    "        return self.linear(x) # final linear layer\n",
    "\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n",
    "  def __init__(self, dim, dropout):\n",
    "      super().__init__()\n",
    "      self.drop = nn.Dropout(dropout)\n",
    "      self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "  def forward(self, x, sublayer):\n",
    "      return x + self.drop(sublayer(self.norm(x)))\n",
    "\n",
    "# I simply let the model learn the positional embeddings in this notebook, since this\n",
    "# almost produces identital results as using sin/cosin functions embeddings, as claimed\n",
    "# in the original transformer paper. Note also that in the original paper, they multiplied\n",
    "# the token embeddings by a factor of sqrt(d_embed), which I do not do here.\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_embed = config.d_embed\n",
    "        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.norm = nn.LayerNorm(config.d_embed)\n",
    "\n",
    "    def forward(self, input, mask=None):\n",
    "        x = self.tok_embed(input)\n",
    "        x_pos = self.pos_embed[:, :x.size(1), :]\n",
    "        x = self.dropout(x + x_pos)\n",
    "        for layer in self.encoder_blocks:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''\n",
    "    def __init__(self, config):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(config.d_embed, config.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.d_ff, config.d_embed)\n",
    "        )\n",
    "        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n",
    "        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # self-attention\n",
    "        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n",
    "        # position-wise fully connected feed-forward layer\n",
    "        return self.residual2(x, self.feed_forward)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.linear = nn.Linear(config.d_embed, num_classes)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.encoder(x, pad_mask)\n",
    "        return  self.linear(torch.mean(x,-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650896401941,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "gxYYnrC-FAgI"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    encoder_vocab_size: int\n",
    "    d_embed: int\n",
    "    # d_ff is the dimension of the fully-connected  feed-forward layer\n",
    "    d_ff: int\n",
    "    # h is the number of attention head\n",
    "    h: int\n",
    "    N_encoder: int\n",
    "    max_seq_len: int\n",
    "    dropout: float\n",
    "\n",
    "def make_model(config):\n",
    "    model = Transformer(config, num_classes).to(DEVICE)\n",
    "    # initialize model parameters\n",
    "    # it seems that this initialization is very important!\n",
    "    for p in model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HdgZPJBoKY2"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1650896401941,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "Ydp6IfBrkYnL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    losses, acc, count = [], 0, 0\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for idx, (x, y)  in  pbar:\n",
    "        optimizer.zero_grad()\n",
    "        features= x.to(DEVICE)\n",
    "        labels  = y.to(DEVICE)\n",
    "        pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "        pred = model(features, pad_mask)\n",
    "\n",
    "        loss = loss_fn(pred, labels).to(DEVICE)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        acc += (pred.argmax(1) == labels).sum().item()\n",
    "        count += len(labels)\n",
    "        # report progress\n",
    "        if idx>0 and idx%50 == 0:\n",
    "            pbar.set_description(f'train loss={loss.item():.4f}, train_acc={acc/count:.4f}')\n",
    "    return np.mean(losses), acc/count\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs):\n",
    "    for ep in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, test_loader)\n",
    "        print(f'ep {ep}: val_loss={val_loss:.4f}, val_acc={val_acc:.4f}')\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            features = x_test.to(DEVICE)\n",
    "            labels  = y_test.to(DEVICE)\n",
    "            pad_mask = (features == PAD).view(features.size(0), 1, 1, features.size(-1))\n",
    "            pred = model(features, pad_mask)\n",
    "            loss = loss_fn(pred,labels).to(DEVICE)\n",
    "            losses.append(loss.item())\n",
    "            acc = (pred.argmax(1) == labels).sum().item()\n",
    "            count = len(labels)\n",
    "    return np.mean(losses), acc/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3022,
     "status": "ok",
     "timestamp": 1650896404954,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "tIau66WRlzRm",
    "outputId": "51942f59-a52e-4aa2-f9f6-def535ee51b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "Transformer                                        --\n",
      "├─Encoder: 1-1                                     --\n",
      "│    └─Embedding: 2-1                              480,000\n",
      "│    └─ModuleList: 2-2                             --\n",
      "│    │    └─EncoderBlock: 3-1                      12,704\n",
      "│    └─Dropout: 2-3                                --\n",
      "│    └─LayerNorm: 2-4                              64\n",
      "├─Linear: 1-2                                      132\n",
      "===========================================================================\n",
      "Total params: 492,900\n",
      "Trainable params: 492,900\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(encoder_vocab_size = vocab_size,\n",
    "                     d_embed = 32,\n",
    "                     d_ff = 4*32,\n",
    "                     h = 1,\n",
    "                     N_encoder = 1,\n",
    "                     max_seq_len = max_seq_len,\n",
    "                     dropout = 0.1\n",
    "                     )\n",
    "model = make_model(config)\n",
    "print(torchinfo.summary(model))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39274,
     "status": "ok",
     "timestamp": 1650896444224,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "Qo7RYNx0lzRm",
    "outputId": "14e319f1-fb24-4411-aac0-8fe983a7f267",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.3117, train_acc=0.8987: 100%|██████████| 938/938 [00:06<00:00, 155.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 0: val_loss=0.2383, val_acc=0.9179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.1898, train_acc=0.9409: 100%|██████████| 938/938 [00:06<00:00, 154.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 1: val_loss=0.2431, val_acc=0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.1631, train_acc=0.9524: 100%|██████████| 938/938 [00:05<00:00, 157.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 2: val_loss=0.2633, val_acc=0.9193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train loss=0.0796, train_acc=0.9615: 100%|██████████| 938/938 [00:05<00:00, 157.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep 3: val_loss=0.2837, val_acc=0.9193\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, test_loader, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBSr0A9noh2i"
   },
   "source": [
    "## News classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1650896444225,
     "user": {
      "displayName": "Hongbin Chen",
      "userId": "14161707569992931612"
     },
     "user_tz": 240
    },
    "id": "uaAPcoPTkYnM",
    "outputId": "25e191cd-854e-490e-872c-769efe32c62d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sci/Tec news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def classify_news(news):\n",
    "    x_text = tokenizer(news.lower())[0:max_seq_len]\n",
    "    x_int = torch.tensor([[word_to_id.get(word, UNK) for word in x_text]]).to(DEVICE)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x_int).argmax(1).item() + 1\n",
    "    print(f\"This is a {ag_news_label[pred]} news\")\n",
    "\n",
    "# The model correctly classifies a theoretical physics news as Sci/Tec news, :-)\n",
    "news = \"\"\"The conformal bootstrapDavid Poland1,2and David Simmons-Duﬃn2*The conformal bootstrap was\n",
    "proposed in the 1970s as a strategy for calculating the properties of second-order phasetransitions.\n",
    "After spectacular success elucidating two-dimensional systems, little progress was made on systems in\n",
    " higher dimensions until a recent renaissance beginning in 2008. We report on some of the main results and\n",
    "  ideas from thisrenaissance, focusing on new determinations of critical exponents and correlation\n",
    "  functions in the three-dimensional Ising and O(N) models.\n",
    "\"\"\"\n",
    "classify_news(news)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Encoder_only_transformer_AG_News_classification.ipynb",
   "provenance": [
    {
     "file_id": "1P7oU2EWQ1Qk17N9NutZv9FV1a_hMGUoR",
     "timestamp": 1647373127550
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
